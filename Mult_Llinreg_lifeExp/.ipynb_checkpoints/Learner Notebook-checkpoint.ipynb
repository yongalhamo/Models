{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression \n",
    "\n",
    "\n",
    "The relationship with a 2-variable linear equation:\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "y = b + w_1.x_1 + w_2.x_2\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "In a vector form:\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "y = b + (w_1 w_2).\\binom{x_1}{x_2}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Where\n",
    "$\n",
    "\\begin{equation}\n",
    "W = (w_1 w_2)\n",
    "\\end{equation}\n",
    "$\n",
    "and\n",
    "$\n",
    "\\begin{equation}\n",
    "X = \\binom{x_1}{x_2}\n",
    "\\end{equation}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_examples(num=1000):\n",
    "    W = [1.0,-3.0] #2 dimentional vector\n",
    "    b = 1.0        #scalar value\n",
    "    \n",
    "    W =np.reshape(W,(2,1))    #2 is number of elements the W has and 1 for number of w\n",
    "    \n",
    "    X=np.random.randn(num, 2)\n",
    "    \n",
    "    y= b + np.dot(x,W) +np.random.randn()\n",
    "    \n",
    "    y = np.reshape(y,(num,1))\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2) (1000, 1)\n"
     ]
    }
   ],
   "source": [
    "X,y = generate_examples()\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03490943  0.06751966] [-4.31304756]\n"
     ]
    }
   ],
   "source": [
    "print(X[0],y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.37355062 -1.19021385] [4.87004405]\n"
     ]
    }
   ],
   "source": [
    "print(X[1],y[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st Function: Initialize Parameters\n",
    "\n",
    "\n",
    "The loss over **m** examples:\n",
    "Mean Squared Error:\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "J = \\frac{1}{2m} \\sum_{i=1}^{m} (y - \\hat{y})^2\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "The objective of the gradient descent algorithm is to minimize this error  value.\n",
    "\n",
    "Gradient Descent Objective is to \n",
    "$\n",
    "\\begin{equation}\n",
    "min(J)\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2nd Function: Forward Pass\n",
    "The gradient descent algorithm:\n",
    "\n",
    "1. Get predictions y_hat for X with current values of W and b.\n",
    "2. Compute the loss between y and y_hat\n",
    "3. Find gradients of the loss with respect to parameters W and b\n",
    "4. Update the values of W and b by subtracting the gradient values obtained in the previous step\n",
    "\n",
    "Let's simplify our linear equation a bit more for an example:\n",
    "$\n",
    "\\begin{equation}\n",
    "y = wx\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Let's plot J as a function of w\n",
    "\n",
    "![Loss vs Param](JvsW.png)\n",
    "\n",
    "The gradients of loss with respect to w:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{dJ}{dw} = \\frac{\\delta{J}}{\\delta{w}} = \\lim_{\\epsilon \\to 0} \\frac{J(w + \\epsilon) - J(w)}{\\epsilon}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 3rd Function:  Compute Loss\n",
    "mean squared error:\n",
    "\n",
    "The loss over **m** examples:\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "J = \\frac{1}{2m} \\sum_{i=1}^{m} (y - \\hat{y})^2\n",
    "\\end{equation}\n",
    "$\n",
    "### 4th Function:  Backward Pass\n",
    "\n",
    "The gradient of loss with respect to bias can be calculated with:\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "\\frac{dJ}{db} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y^{(i)}} - y^{(i)})\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "\\frac{dJ}{dW_j} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y^{(i)}} - y^{(i)}).x_j^{(i)}\n",
    "\\end{equation}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel:\n",
    "    \n",
    "    def __init__(self, num_features):\n",
    "        self.num_features = num_features\n",
    "        self.W = np.random.randn(num_features, 1)\n",
    "        self.b = np.random.randn()\n",
    "        \n",
    "    def forward_pass(self, X):\n",
    "        y = self.b + np.dot(X, self.W)\n",
    "        return y\n",
    "    \n",
    "    def compute_loss(self, y, y_true):\n",
    "        loss = np.sum(np.square(y - y_true))\n",
    "        return loss/(2*y.shape[0])\n",
    "    \n",
    "    def backward_pass(self, X, y_true, y_hat):\n",
    "        m = y_hat.shape[0]\n",
    "        db = np.sum(y_hat - y_true)/m\n",
    "        dW = np.sum(np.dot(np.transpose(y_hat - y_true), X), axis=0)/m\n",
    "        return dW, db\n",
    "    \n",
    "    def update_params(self, dW, db, lr):\n",
    "        self.W = self.W - lr * np.reshape(dW, (self.num_features, 1))\n",
    "        self.b = self.b - lr * db\n",
    "        \n",
    "    def train(self, x_train, y_train, iterations, lr):\n",
    "        losses = []\n",
    "        for i in range(iterations):\n",
    "            y_hat = self.forward_pass(x_train)\n",
    "            dW, db = self.backward_pass(x_train, y_train, y_hat)\n",
    "            self.update_params(dW, db, lr)\n",
    "            loss = self.compute_loss(y_hat, y_train)\n",
    "            losses.append(loss)\n",
    "            if i % int(iterations/10) == 0:\n",
    "                print('Iter: {}, Current loss: {:.4f}'.format(i, loss))\n",
    "        return losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.50911788]\n",
      " [-1.93642004]]\n",
      "1.3156616675120554\n"
     ]
    }
   ],
   "source": [
    "LinearModel = LinearModel(2)  #Intialize Model taking 2 features\n",
    "#test the model\n",
    "\n",
    "print(LinearModel.W)\n",
    "print(LinearModel.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1) [1.16714223]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#2nd function:first step of gradient descent\n",
    " \n",
    "y_hat = LinearModel.forward_pass(X)\n",
    "print(y_hat.shape,y_hat[0])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.406707059399394"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3rd function : calculate loss\n",
    "y_hat = LinearModel.forward_pass(X)\n",
    "loss = LinearModel.compute_loss(y_hat,y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.65230137 -2.19092547] -0.24803837449208416\n"
     ]
    }
   ],
   "source": [
    "#backward pass test\n",
    "X,y = generate_examples()\n",
    "y_hat = LinearModel.forward_pass(X)\n",
    "\n",
    "dw,db = LinearModel.backward_pass(X,y,y_hat)\n",
    "print(dw,db)\n",
    "#db gradient with loss with respect to b\n",
    "#bw gradient with respect to w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train = generate_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 9: Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
